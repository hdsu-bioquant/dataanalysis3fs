---
title: "Exercise Sheet 7"
author: "Maiwen Caudron-HergerCarl Herrmann"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)
## Global options
options(max.print="120")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=120)
opts_knit$set(root.dir  = "~/")
```

```{r, echo = FALSE}
all.aml = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.cleaned.csv',header=TRUE)
all.aml.anno = read.delim("http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.anno.cleaned.csv", header=TRUE)
#
# we convert all.aml into a data matrix rather than a data.frame
all.aml = data.matrix(all.aml)
i.all = grep('ALL', all.aml.anno$ALL.AML)
```

# 0. Recap of the previous Sheet

In the previous Exercise Sheet we have looked at **hypothesis testing**, learned how the process works and then learned about the t-test function.

# 1. Introduction and Objectives

In this Exercise Sheet you will learn more about hypothesis testing, specifically about how to deal with different distributions and proportion testing. You should be familiar with both these problems from the lecture.

# 2. What if the data is not normally distributed?

At the beginning, we checked if the distribution of the data in the samples corresponds to a normal distribution.

Check again the distribution of the expression values for the gene FOSB
```{r}
expression = all.aml['FOSB',]
hist(expression,breaks = 30);qqnorm(expression);qqline(expression)
```

This looks everything but normal! In that case, we cannot apply the t.test, but need to apply a **non-parametric test** called the *Wilcoxon test* (check the lecture notes!). This test is performed not on the *values* (like the t-test) but on the *ranks* of these values (remember the difference between the Pearson's and the Spearman's correlations!)

```{r}
exp.all = all.aml['FOSB',i.all]
exp.aml = all.aml['FOSB',-i.all]
wilcox.test(exp.all,exp.aml)
```

Compare the obtained p-value with the p-value obtained if we would have used the t-test:

```{r}
exp.all = all.aml['FOSB',i.all]
exp.aml = all.aml['FOSB',-i.all]
t.test(exp.all,exp.aml)
```

The p-values are very different!! So is the difference of expression between ALL and AML patients for this gene significant or not taking $\alpha=0.05$?

Here, we **cannot** trust the t-test due to the non-normality of the data! Hence, the correct p-value is the one from the Wilcoxon test.

### Going further...

What does it mean, when we say that we cannot trust the t-test? Remember that the significance value $\alpha$ represents the **false-positive rate (FPR)**. Hence, with an $\alpha$ of 5\%, if $H_0$ is true, we have a 5\% risk of rejecting $H_0$ (False-positive).

Let us suppose that the $H_0$ Hypothesis: "The expectation values of two distributions are equal" holds. For example, we can generate 2 random samples, drawn from the same distribution, and perform a t-test. Here, $H_0$ would hold! What would the p-value be? Well, if we perform this 10000 times, we would obtain a uniform distribution:

```{r}
set.seed(123)
alpha = 0.05
p = sapply(1:10000,function(i) {
  x = rnorm(10);y = rnorm(10)
  t.test(x,y)$p.value
})
hist(p,breaks=20);abline(v=alpha,col='red',lwd=3)
```

Approximately 5\% of these p-values are smaller than $\alpha$ (see red line):

```{r}
sum(p<alpha)/length(p)
```

These would be **False positives**. So here, we indeed have $\alpha$ = False-positive rate!

This also holds true for different values of $\alpha$:

```{r}
alphas = c(0.01,0.02,0.03,0.04,0.05,0.07,0.1,0.2)
fpr = sapply(alphas,function(alpha) {
  sum(p<alpha)/length(p)
})
plot(alphas,fpr,xlab='alpha',pch=19,col='red');abline(0,1,lwd=2,lty=2,col='lightgrey')
```

Now what if the distributions of values had same expectation values, but were not normaly distributed? Let's sample x and y from a t-Distribution, instead of a normal distribution:

```{r}
p = sapply(1:10000,function(i) {
  x = rt(10,df=1);y = rt(10,df=1)
  t.test(x,y)$p.value
})
hist(p,breaks=20);abline(v=alpha,col='red',lwd=3)
```

No longer a uniform distribution!! How often would we reject $H_0$ here?

```{r}
sum(p<alpha)/length(p)
```

Not exactly $\alpha$ anymore... What if we user different values of $\alpha$?


```{r}
alphas = c(0.01,0.02,0.03,0.04,0.05,0.07,0.1,0.2)
fpr = sapply(alphas,function(alpha) {
  sum(p<alpha)/length(p)
})
plot(alphas,fpr,xlab='alpha',pch=19,col='red');abline(0,1,lwd=2,lty=2,col='lightgrey')
```

So in the case of a non-normal distribution of the data, the false-positive rate (FPR) is no longer equal to the significance level $\alpha$... **Hence, we can no longer control the FPR with $\alpha$!**



***

#### Exercise set A


1. Check the expression of a random gene. Are the values normally distributed? 

2. Run both tests on this gene. Is there a difference between p-values? What did you expect? 

3. Write a code that runs both tests on every gene and registers each p-value in a vector.

4. Do a scatter plot of -log10(Pvalue of t-test) vs. -log10(Pvalue of wilcoxon test); are there strong deviations?

***

# 3. testing proportions

The proportion test (Fisher Extact Test or chi2 test) are used to investigate the relationship between 2 categorical variables, starting from a contingency table. We will use a dataset with clinical informations about breast cancer patients.

```{r}
dat = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/gbsg_ba_ca.dat', stringsAsFactors = FALSE)
```


Check which variables in this dataset are categorical/ordinal/numerical. We can now check if there is a significant relationship between some variables. For example, we can verify if the choice of treatment with tamoxifen (variable `hormon`) is related to the pre-/post-monopausal status (variable `meno`)

First, we can build the contingency table table for these 2 variables:
```{r}
## build contingency table
table(dat$meno,dat$hormon)
```

***
#### Exercise set B


1. What test would you use for the following questions? 
  + A lotion company has to figure out whether their last product is more likely to give men acne rather than females
  + The department of education wants to find out whether social science students have higher grades than science students
  + A biologist needs to find out whether a specific gene is more likely to be silenced in lactose intolerant people.

2. Compute the odds-ratio (OR) for the two variables *meno* and *hormon* from the dataset dat. How would the odds-ratio look like if you would transpose the matrix?

***
Now we can run the Fisher Exact Test (FET), or the chi2-test.

```{r}
## build contingency table
tab = table(dat$meno,dat$hormon)
#
## run the FET
fisher.test(tab,alternative = 'greater')
```

Check if your computation of the odds-ratio is right! (See Exercise set B.2)
Compute also the two 1-sided test.


Now we want to verify the impact of age on the grade of the tumor. We categorize the patients in under and over 40 year groups, and perform a chi-squared test:

```{r}
## contingency table
tab = table(dat$age>40,dat$grade)
tab
##
chisq.test(tab)
```

> Determine the table of expected counts, if there was no relationship between age and grade
> You can do this by hand, or try to do it using R!


#### Exercise set C


1. A pharmaceutical company has a new corona vaccine that works on 178 out of 200 patient. 75% of the batch had been previously vaccinated against corona with another previous vaccine, and on 143 of these did the vaccine work. Is there a significant disproportion to say that the vaccine has a higher chance of working on people who are already vaccinated against corona if our p-value threshold is 0.05?


# 4. Power of a test

We have seen that $\alpha$ controls the **False-positive rate**. False positive means seing something that is not there... 

The other type of error are **False-negatives**, i.e. **not seeing** something that is there!


Let us imagine the following scenario:

## Hunting penguins in Antarctica...

You have been sent on a research mission in Antarctica, to study two penguin populations:
1. the King penguins
2. the Emperor penguins

Both species are similar, but they have different average weight: $w_K = 15\,kg$, $w_E=16\,kg$. The standard deviation for both is $3.5\,kg$

```{r}
wK = 15
wE = 16
s = 3.5
n = 10
```


You are going to an area where **mostly King penguin live**; you are catching 10 penguins a day, weight them, and compute the average weight of the daily sample. Since penguins don't mix, you are always catching individuals from the same species (King or Emperor).

The $H_0$ hypothesis is: *"The penguins caught are King penguins, not Emperor penguins"*

Here are the results of your catch from the first 5 days of your mission

```{r}
mean_weights = c(16.64268,17.69074,15.07522,14.52752,15.76787)
```

What is the expected distribution of mean weights over samples of $n=10$ penguins?

```{r}
x = seq(12,18,by =0.01)
y = dnorm(x,mean=wK,sd=s/sqrt(n))
plot(x,y,type='l',lwd=3,col='red')
```

**Note the $\sqrt{n}$ factor in the standard deviation!!**


When would you reject the $H_0$ (one sided test, $\alpha=0.05$)? 
```{r}
rej_lim = qnorm(p=0.05,mean=wK,sd=s/sqrt(n),lower.tail = FALSE)
rej_lim
```

Whenever your measured mean weight lies above this, you would reject $H_0$ (so-called **positive** events; otherwise, would not reject $H_0$ (**negative** events).

```{r}
CI_K_high = 15+1.96*3.5/sqrt(10)
plot(wK,1,ylim=c(0,3),xlim=c(10,20),pch=19,cex=1.4,col='red');segments(10,1,CI_K_high,1,col='red');points(mean_weights,rep(1.5,length(mean_weights)))
```

So day 2 might be a sample of Emperor penguins , but you are pretty confident about the other 4 samples.

However, it turns out that the helicopter wrongly brought you to an area in which there are **only** Emperor penguin living! So all caught samples were composed on Emperor penguins, and $H_0$ is **not valid** for **any** of these 5 samples!

Hence, you failed to reject $H_0$ for 4 out of 5 samples: 80\% **false-negatives** !

Let's determine how often that would happen if we consider 1000 samples made of Emperor penguins:

```{r}
### Parameters
wK = 15
wE = 16
n = 10
s = 3.5
alpha = 0.05

### 1000 samples of n penguins each
mean_weights  = sapply(1:1000,function(i) {mean(rnorm(n,mean=wE,sd=s))})


### rejection limit for H0 : "samples are King, not Emperor penguins"
rej_lim = qnorm(alpha,mean=wK,sd = s/sqrt(n),lower.tail = FALSE)

### for how many sample did I falsely **not** reject H0?

beta = sum(mean_weights < rej_lim)/length(mean_weights)
beta

```

This is a huge False-negative rate!!

> Try to find a combination of effect size (i.e. difference between the expectation values for the weights), significance level, sample size, so that the beta value is lower than 30%
> For a given effect size and significance level, plot the power of the test as a function of the sample size

The next code helps visualize what is going on:

```{r}
source('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/helper.R')
n =10
h0h1(wK,wE,s,n,alpha)
```



