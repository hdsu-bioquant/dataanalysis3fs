---
title: "Exercise Sheet 3 - Hierarchical Clustering / PCA"
author: "Carl Herrmann - Maiwen Caudron-Herger"
output: html_document
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)
## Global options
options(max.print="120")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=120)

```


```{r, echo=FALSE, eval = TRUE}

#### This is all pre-processing of the BRCA dataset
#### RUN THIS CHUNK AT THE BEGINNING


brca.exp = readRDS(url('https://www.dropbox.com/s/m6nfxul95borupz/brca.exp_large.rds?dl=1'))
brca.anno = readRDS(url('https://www.dropbox.com/s/9xlivejqkj77llc/brca.anno.rds?dl=1'))


## we store the original data.frame into a new variable
## so that we can restore it in case we want to start with the unmodified data.frame
brca.original = brca.exp


##Thresholding 

q95 = quantile(data.matrix(brca.exp),probs=0.95)

## set all values above to this value
brca.exp[brca.exp>q95] = q95

## Homogenization and Scaling

## take log2
brca.exp = log2(brca.exp+1) ## why do we add +1 ??

## scaling
brca.exp = scale(brca.exp)

```

# 0. Recap of the last sheet

In the last sheet we have kept working with our diabetes dataset by learning how to **measure centrality** and other statistical information such as **quantiles**. Furthermore, we have learned functions to measure correlation between variables and explored different methods to do so. Finally, on a new dataset, we have moved over to the concept of unsupervised learning and used one of its most simple but effective methods, k-means clustering.

**Recommendation:** in order to avoid going through several exercise sheets in the future create now a R file where you save all of the code from the previous chunk that we use to download and prepare our data. \ 
You can also save the environment containing all the needed variables as an .RData file or separately save a variable of interest as an .rds file. To save your environment, you can use the save icon of the environment panel, and later load this environment using `load()`. You can save a specific variable, for example the dataset "brca.exp" (after all cleaning steps) like this:

```{r, eval= FALSE}
saveRDS(brca.exp, file = "brca_exp.rds") #save the dataset "brca.exp" as an .rds file
```

Whenever you need it, you can load the dataset using the function `readRDS()`:

```{r, eval= FALSE}
brca.exp = readRDS("brca_exp.rds")
```

# 1. Objectives

In this sheet we will conclude the topic of **unsupervised learning** by looking at **hierarchical clustering** and **Principal Component Analysis (PCA)**.

# 2. Hierarchical clustering

Clustering is a method by which we group together similar observations while separating out the dissimilar ones. We will cluster our samples from the cancer dataset to see which samples cluster together or separately. Hierarchical clustering does not generate discrete clusters of datapoints, but rather creates a dendrogram that indicates the magnitude of similitude between samples. Once again is up to the Data Scientist to decide the right amount of clusters.

### Using the most variable, thus informative genes

When performing clustering, we usually reduce the number of genes used, as some of them are not informative. For example, genes that show a mostly constant expression across all samples will not be useful to distinguish the samples, right? One simple method is to select genes that show a **high variance** across all samples.

```{r}
## compute the variance for all genes across all samples
topVar = apply(brca.exp, 1, var)
summary(topVar)
```

We now want to find the top 25% with the highest variance

```{r}
## what is the 75% quantile of the variance?
q75 = quantile(topVar, probs = 0.75)
q75
```

So let us select all rows (genes) with a variance higher than or equal to `q75`:

```{r}
i.topvar = which(topVar >= q75)

## only select the gene with a variance in the top 25%
brca.exp.topVar = brca.exp[i.topvar,]
dim(brca.exp.topVar) # 250 genes and 200 samples
```


### Computing the correlation between all patients (columns)

```{r}
## Creating a correlation based distance matrix
cor.mat = cor(brca.exp.topVar, method="spearman")
```

Let us display the correlation matrix as a heatmap, using the `pheatmap` function:

```{r}
library(pheatmap)
pheatmap(cor.mat, show_rownames = FALSE, show_colnames = FALSE)
```

Each cell of this heatmap represents the correlation value between the sample in the row and the sample in the column. The correlation of a sample to itself is always 1 (red diagonal).

The function automatically determines the clustering trees of the rows and columns (which are identical, since the correlation matrix is symmetrical!)

### Plotting the dendrogram along with clinical annotations

This is a nice representation, but in order to interpret this clustering, we need to add some additional (clinical) information to interpret the clustering structure. To do this, we use an annotation data frame containing as columns a number of clinical features.

The clinical annotation is stored in the `brca.anno` data frame.

```{r}
head(brca.anno)
```

We can now plot again the heatmap, using the annotation dataframe to add additional information

```{r}
pheatmap(cor.mat,annotation_row = brca.anno,
         show_rownames = FALSE, show_colnames = FALSE)
```

> How would you interpret this dendrogram? Do the clusters you observe make any sense? What are the parameters by which the samples cluster together? How many meaningful clusters can you observe? Do you see any relation between the distribution of the data and your clusters ? 

> The function `pheatmap` accepts a parameter clustering_method to indicate alternative linkage methods; try other linkage methods (check which are available with the pheatmap help page, which can be accessed by typing `?pheatmap` in the console!)


# 3. Dimensionality reduction - PCA

We will now use principal component analysis to explore the same dataset, and identify directions (i.e. principal components) with maximal variance. Principal components analysis finds n-dimensional vectors (Principal Components) in the direction of the largest variance, thereby allowing you to describe an n-dimensional dataset with just a few dimensions.

```{r}
pca = prcomp(t(brca.exp.topVar), center = FALSE, scale = FALSE) # We set these as false as we have already scaled our data
summary(pca)
```

> How many principal components do you obtain? Compare this to the dimension of the matrix using the `dim()` function! 

> What would happen if you would not transpose the matrix with t(...) in the prcomp function?

Principal components are ranked by the amount of variance that they explain. This can be visualized using a **scree plot**, indicating how much variance each PC explains: the  **standard deviation** explained by each principal component is contained in the `pca$sdev` vector:

```{r}
pca$sdev
```

We see that the standard deviation is indeed going down! Let us now plot the proportion of total variance explained by each PC

```{r}
variance = (pca$sdev)^2
prop.variance = variance/sum(variance)
names(prop.variance) = 1:length(prop.variance)
barplot(prop.variance[1:20],ylab='Proportion of variance') # we only plot the first 20 PCs
```

Principal component analysis represents each data point (here: patient) in a new space in which the coordinates are principal components. Compare the following 2 outputs:

```{r}
## Initial data
head(t(brca.exp.topVar))
```

```{r}
## after PCA
head(pca$x)
```

We can now display the data points (i.e. patients) in the first two principal components. In addition, we can color the dots according to certain clinical parameters:

```{r}
## define a color vector for annotations
color = c('grey','red','blue')


## the ER_status column in the annotation dataframe is a factor, with 3 levels
## Remember that a factor will be treated as an integer number, so we can use it to select one of the elements in the color vector
cb = color[brca.anno$ER_status]


plot(pca$x[,1], pca$x[,2], 
     col= cb, pch=19,
     xlab='PC1',ylab='PC2');
legend('topleft',legend=levels(brca.anno$ER_status),pch=19,col=color)
```

> Choose different PCs for this plot. Can you still observe the two clusters corresponding to the ER_status of the patients? 


## EXERCISES

#### Exercise 1

*For this exercise set it is given that all the data cleanup steps have been taken, you don't need to put them in the results.*

1.  Make a heatmap of the reduced matrix "brca.exp.topVar" using the `pheatmap()` function of the `pheatmap` library. Check for parameters that might change the style of the heatmap (column names, row names, etc..). How is this heatmap different from the heatmap in section 2?

2.  Repeat the selection of top variable genes, but using the median absolute deviation (or MAD, see lecture) using the `mad()` function instead of the `sd()` function, and store into a matrix `brca.ex.topMAD`

3.  Extract the gene names of `brca.exp.topVar` and `brca.ex.topMAD` and check how many overlap using the `intersect()` function.

```{r}
### SOLUTION

# 1.
library(pheatmap)
pheatmap(brca.exp.topVar, show_rownames = FALSE, show_colnames = FALSE)

# 2.
## compute the MAD for all genes across all samples
topMAD = apply(brca.exp, 1, mad)

## what is the 75% quantile?
q75m = quantile(topMAD, probs = 0.75)

i.topmad = which(topMAD >= q75m)

## only select the genes in the top 25%
brca.exp.topMad = brca.exp[i.topmad,]
dim(brca.exp.topMad)

# 3.
top.genes.Var = rownames(brca.exp.topVar)
top.genes.Mad = rownames(brca.exp.topMad)

length(intersect(top.genes.Mad,top.genes.Var))
```


#### Exercise 2 - Hierarchical clustering

In section 2, we have computed a correlation matrix, and used this matrix to build a clustering tree.

1.  Create a 4x4 distance matrix, following the example below (but with other values!); beware that a distance matrix should be **symmetrical**!!

```{r}
## this is the example from the lecture slides
d = matrix(c(0,2,3,5,2,0,1,4.1,3,1,0,4,5,4.1,4,0),nrow=4)
rownames(d) = letters[1:4]
colnames(d) = letters[1:4]
pheatmap(d)
```

Try different linkage methods using the `clustering_method` parameter to see if the topology of the dendrogram changes!

2.  Try building a distance matrix which would lead to different topologies of the dendrogram, depending on which linkage method is used! Show the dendrograms built with different linkage methods!

```{r}
### SOLUTION
d = matrix(c(0,4,2,6,
             4,0,3,4,
             2,3,0,1,
             6,4,1,0),nrow=4, byrow = TRUE)
rownames(d) = letters[1:4]
colnames(d) = letters[1:4]

# here we have 2 different topologies!
pheatmap(d, clustering_method = 'complete')
pheatmap(d, clustering_method = 'single')

```

#### Exercise 3 - PCA

1.  Display the patients in the first two principal components (available in `pca$x`) using `plot()`. Color the patients in the PCA plot according to `HER2_status`; you should check first how many levels this annotation has using `levels()` to determine how many colors you will need!

2.  Color the patients in the PCA plot according to `Classification`; you will probably need to define some more colors... You can check available colors [here](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)


```{r}
### SOLUTION

# 1.
## how many levels do we have for `HER2_status`?
length(levels(brca.anno$HER2_status)) # 3 levels

## define a color vector for annotations
color = c('grey','red','blue')

## the HER2_status column in the annotation dataframe is a factor, with 3 levels
## Remember that a factor will be treated as an integer number, so we can use it to select one of the elements in the color vector
cb = color[brca.anno$HER2_status]

plot(pca$x[,1], pca$x[,2], 
     col= cb, pch=19,
     xlab='PC1',ylab='PC2',
     main='BRCA patients according to HER2 status');
legend('topleft',
       legend=levels(brca.anno$HER2_status),pch=19,col=color)


# 2. 
length(levels(brca.anno$Classification)) # 5 levels

## define a color vector for annotations
color = c('grey','red','blue', "green", "purple")

## select elements of the color vector according to the Classification
cb = color[brca.anno$Classification]

plot(pca$x[,1], pca$x[,2], 
     col= cb, pch=19,
     xlab='PC1',ylab='PC2',
     main='BRCA patients according to Classification');
legend('topleft',
       legend=levels(brca.anno$Classification),pch=19,col=color)
```


#### Going further *(expert)* 

Instead of performing the k-means on the whole gene expression matrix, we can run k-means on the space in which each patient is represented by the first k principal components (available in `pca$x`)

1.  Run k-means with different numbers of clusters (1-10) on the patients using the first 2, 4, 6,... principal components (i.e. the first columns of `pca$x`). Use the elbow method to evaluate how the within sum of squares (WSS) evolves. What is the optimal number of clusters? 
*Hint*: run a `sapply` loop with the `kmeans()` function and choose the WSS from the `$tot.withinss` attribute (you can refer to the end of last week's exercise sheet 02 for an example). Plot the WSS using the `plot()` function. 

2.  Represent the patients in the PCA plot as previously, but color them according to the cluster they belong to! Run kmeans with two clusters for this and choose the cluster assignment from the `$cluster` attribute. 


```{r}
### SOLUTION

# 1. 

## Number of PCs to select
n = 5 # you can choose different values for n, depending on the number of PCs that you want to select
data = pca$x[,1:n]

# we can run a loop to see how the within sum of squares (WSS) evolves (see last week`s exercise sheet): 
wss = sapply(1:10,function(k) { 
  kmeans(data, centers = k, nstart = 10)$tot.withinss
})

plot(1:10, wss, type='b',pch=19,xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")



## We can also use the function `fviz_nbclust()` from the "factoextra" package
## Therefore, you have to first install the corresponding package by typing `install.packages("factoextra")` in the console
## Then, the package has to be activated: 
library(factoextra)

## Number of PCs to select
n = 5 # you can choose different values for n, depending on the number of PCs that you want to select
data = pca$x[,1:n]

# Silhouette and WSS plot
fviz_nbclust(data, kmeans, method = "wss")
fviz_nbclust(data, kmeans, method = "silhouette")
```

```{r}

# 2.

## reduce the dataset to the first n PCs
n = 5
data = pca$x[,1:n]

## perform kmeans on this new data, with k=2
km = kmeans(data,centers=2,nstart=50)

## define the color vector
colclust = c('red','blue')[km$cluster] 

plot(pca$x[,1], pca$x[,2], 
     col= colclust, pch=19,
     xlab='PC1',ylab='PC2',
     main='BRCA patients according to cluster');
legend('topleft',
       legend=c("Cluster 1", "Cluster 2"), pch=19, col=c('red','blue'))
```


