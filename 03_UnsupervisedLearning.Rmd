---
title: "Exercise Sheet 3 - Hierarchical Clustering / PCA"
author: "Carl Herrmann - Maiwen Caudron-Herger"
output: html_document
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)
## Global options
options(max.print="120")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=120)

```

```{r, echo=FALSE, eval = TRUE}

#### This is all pre-processing of the all/aml dataset
#### RUN THIS CHUNK AT THE BEGINNING

all.aml.exp = readRDS(url('https://www.dropbox.com/s/eqjf32cklgtkfnr/all.aml.exp.rds?dl=1'))
all.aml.anno = read.delim("https://www.dropbox.com/s/ejgf6mu5ca9uhv2/ALLannotation.txt?dl=1", header=T)

## we store the original data.frame into a new variable
## so that we can restore it in case we want to start with the unmodified data.frame
all.aml.exp.original = all.aml.exp

## set all values below 100 to exactly 100
## set all values above 16000 to exactly 16000
all.aml.exp[all.aml.exp < 100] = 100
all.aml.exp[all.aml.exp > 16000] = 16000

## for each gene, compute min and max values
min.val.gene = apply(all.aml.exp, 1, min)
max.val.gene = apply(all.aml.exp, 1, max)

## here, we identify genes for which the fold change between max and min is smaller than 5
## or (symbol |) if the difference of expression between max and min is smaller than 500
genes.remove = which(max.val.gene/min.val.gene <=5 | max.val.gene - min.val.gene <= 500)

## remove those genes
all.aml.exp = all.aml.exp[-genes.remove,]

## apply a log transformation to all values
all.aml.exp = log10(all.aml.exp)

## scale the data column-wise
all.aml.exp = scale(all.aml.exp)

## select ALL patients
i.all = which(all.aml.anno$ALL.AML == "ALL")
all.dat = all.aml.exp[,i.all] 
med.exp.all = apply(all.dat, 1, median)

```

# 0. Recap of the last sheet

In the last sheet we have kept working with our diabetes dataset by learning how to **measure centrality** and other statistical information such as **quantiles**. Furthermore, we have learned functions to measure correlation between variables and explored different methods to do so. Finally, on a new dataset, we have moved over to the concept of unsupervised learning and used one of its most simple but effective methods, k-means clustering.

**Recommendation:** in order to avoid going through several exercise sheets in the future create now a R file where you save all of the code from the previous chunk that we use to download and prepare our data.

# 1. Objectives

In this sheet we will conclude the topic of **unsupervised learning** by looking at **hierarchical clustering** and **Principal Component Analysis (PCA)**.

# 2. Hierarchical clustering

Clustering is a method by which we group together similar observations while separating out the dissimilar ones. We will cluster our samples from the cancer dataset to see which samples cluster together or separately. Hierarchical clustering does not generate discrete clusters of datapoints, but rather creates a dendrogram that indicates the magnitude of similitude between samples. Once again is up to the Data Scientist to decide the right amount of clusters.

### Using the most variable, thus informative genes

When performing clustering, we usually reduce the number of genes used, as some of them are not informative. For example, genes that show a mostly constant expression across all samples will not be useful to distinguish the samples, right? One simple method is to select genes that show a **high variance** across all samples.

```{r}
## compute the variance for all genes accross all samples
topVar = apply(all.aml.exp, 1, var)
summary(topVar)
```

We now want to find the top 25% with the highest variance

```{r}
## what is the 75% quantile of the variance?
q75 = quantile(topVar, probs = 0.75)
q75
```

So let us select all rows (genes) with a variance higher than `q75`:

```{r}
i.topvar = which(topVar >= q75)

## only select the gene with a variance in the top 25%
all.aml.exp.topVar = all.aml.exp[i.topvar,]
dim(all.aml.exp.topVar)
```

#### Exercise set A

*For this exercise set it is given that all the data cleanup steps have been taken, you don't need to put them in the results.*

1.  Let's recap what we learned in the last Exercise Sheet first! Use the following dataset and calculate the mean, median and standard deviation of each variable.

```{r}
Dat = data.frame(c(1,2,3,4,5,6,7,8,9,10), c(rnorm(10, 15, 4)), c(rpois(10, 3)), c(rbinom(10, 1, 0.4)))
colnames(Dat) = c("X", "Variable A", "Variable B", "Variable C")
```

2.  Calculate correlation (spearman) between all Variables of Dat in relation to x. Does this method make sense for all variable pairs?

> Variability of genes among samples can also be measured using standard deviation or median absolute deviation.

### Computing the correlation between all patients (columns)

```{r}
## Creating a correlation based distance matrix
cor.mat = cor(all.aml.exp.topVar, method="spearman")
```

Let us display the correlation matrix as a heatmap, using the `pheatmap` function:

```{r}
library(pheatmap)
pheatmap(cor.mat)
```

Each cell of this heatmap represents the correlation value between the sample in the row and the sample in the column. The correlation of a sample to itself is always 1 (red diagonal).

The function automatically determines the clustering trees of the rows and columns (which are identical, since the correlation matrix is symmetrical!)

### Plotting the dendrogram along with clinical annotations

This is a nice representation, but in order to interpret this clustering, we need to add some additional (clinical) information to interpret the clustering structure. To do this, we will create an annotation data frame containing as columns a number of clinical features.

The clinical annotation is stored in the `all.aml.anno` data frame.

```{r}
head(all.aml.anno)
```

Since the rownames already indicate the patient number, we can remove the column `Samples`:

```{r}
## we remove the first column
all.aml.anno = all.aml.anno[,-1]
head(all.aml.anno)
```

We can now plot again the heatmap, using the annotation dataframe to add additional information

```{r}
pheatmap(cor.mat,annotation_row = all.aml.anno,annotation_col = all.aml.anno)
```

> How would you interpret this dendrogram. Does the clusters you observe make any sense? What are the parameters by which the samples cluster together? How many meaningful clusters can you observe? Do you see any relation between the distribution of the data and your clusters ? The function `pheatmap` accepts a parameter clustering_method to indicate alternative linkage methods; try other linkage methods (check which are available with the pheatmap help page!)

#### Exercise set B

In the previous part, we have computed a correlation matrix, and used this matrix to build a clustering tree.

1.  Create a 4x4 distance matrix, following the example below (but with other values!); beware that a distance matrix should be symmetrical!!

```{r}
## this is the example from the lecture slides
d = matrix(c(0,2,3,5,2,0,1,4.1,3,1,0,4,5,4.1,4,0),nrow=4)
dd = as.dist(d)
pheatmap(dd)
```

Now build the dendrogram using the `hclust` , `as.dendrogram` and `plot` functions.

2.  Try building a distance matrix which would lead to different topologies of the dendrogram, depending on which linkage method is used! Show the dendrograms built with different linkage methods!

# 3. Dimensionality reduction - PCA

We will now use principal component analysis to explore the same dataset, and identify directions (i.e. principal components) with maximal variance. Principal components Analysis finds n-dimensional vectors (Principal Components) in the direction of the largest variance, thereby allowing you to describe an n-dimensional dataset with just a few dimensions. Think back of how you could describe most of the distribution from exercise set B with just one vector, even though the data was 2-dimensional.

```{r}
pca = prcomp(t(all.aml.exp.topVar), center = F, scale. = F) # We set these as false as we have already scaled our data
print(pca)
```

> How many principal components do you obtain? Compare this to the dimension of the matrix using the `dim` function! What would happen if you would not transpose the matrix with t(...) in the prcomp function?

Principal components are ranked by the amount of variance that they explain. This can be visualized using a **scree plot**, indicating how much variance each PC explains: the proportion of **standard deviation** explained by each principal component is contained in the `pca$sdev` vector:

```{r}
pca$sdev
```

We see that the standard deviation is indeed going down! Let us now plot the proportion of total variance explained by each PC

```{r}
variance = (pca$sdev)^2
prop.variance = variance/sum(variance)
names(prop.variance) = 1:length(prop.variance)
barplot(prop.variance[1:20],ylab='Proportion of variance') # we only plot the first 20 PCs
```

We can now display the data points (i.e. patients) in the first two principal components. In addition, we can color the dots according to certain clinical parameters:

```{r}
## define color for annotations
color = c('green','red')
cb = apply(all.aml.anno,2,function(x) {color[as.factor(x)]})

# create a color bar corresponding to the ALL and AML patients. We use the function ifelse() which takes a "condition" to test (TRUE or FALSE) and returns a result if (if) TRUE or (else) another result. It looks like ifelse(condition, result_condition_TRUE, result_condition_FALSE). Here we create a vector with two colors, one color for ALL patients and another color for AML patients.  
cb1 <- ifelse(all.aml.anno$ALL.AML == "ALL", "forestgreen", "firebrick")

plot(pca$x[,1], pca$x[,2], 
     col= cb1, pch=19,
     xlab='PC1',ylab='PC2') 
```

#### Exercise set C

**Save the code you use for the exercises!**

*For this exercise set it is given that all the data cleanup steps have been taken, you don't need to put them in the results.*

1.  Color the patients according to the distinction between bone marrow samples and peripheral blood, then redo the plot by including some other PCs (e.g. PC1 vs. PC3)
2.  Color the patients according to the distinction between genders (fourth column) then redo the plot by including some other PCs (e.g. PC1 vs. PC3). Are there any groups? If there were, what could this mean?
3.  Run k-means clustering by using PC1 and PC2, try different amounts of clusters and use the elbow kink method to determine the best amount.
